{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35455b7",
   "metadata": {},
   "source": [
    "# DNN Assignment\n",
    "In this assignment you are working together with your teammates from the second project. You will apply your new knowledge about dense neural networks to the data from your ML project to investigate, if you can make further improvements on prediction performance. Your data is (hopefully) already cleaned and transformed (this was part of your ML project) such that you can focus fully on feeding it to your neural network. Use TensorFlow 2.x in this assignment as it makes training with real-life data much more easier with many implemented features (e.g. early-stopping, TensorBoard, regularization, etc.). \n",
    "\n",
    "In this notebook you will learn\n",
    "- how to apply a neural network to your own data using TensorFlow 2.x\n",
    "- how to tune the network and monitor learning\n",
    "- how to train several networks and ensemble them into a stronger model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f967c6d",
   "metadata": {},
   "source": [
    "# Module loading\n",
    "Load all the necessary packages for your assignment. We give you some modules in advance, feel free to add more, if you need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e0d764d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "    \n",
    "print('Using TensorFlow version: %s' % tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28b25ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q git+https://github.com/tensorflow/docs\n",
    "    \n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "import datetime, time, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00bc3602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline # focus on this one\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score, recall_score, precision_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, recall_score, roc_auc_score, f1_score, roc_curve, r2_score, fbeta_score,cohen_kappa_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a12ef62",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "Load here your data from your ML project. You can use either `pandas` or `numpy` to format your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9eb56623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Place_ID</th>\n",
       "      <th>target</th>\n",
       "      <th>target_min</th>\n",
       "      <th>target_max</th>\n",
       "      <th>target_variance</th>\n",
       "      <th>target_count</th>\n",
       "      <th>precipitable_water_entire_atmosphere</th>\n",
       "      <th>relative_humidity_2m_above_ground</th>\n",
       "      <th>specific_humidity_2m_above_ground</th>\n",
       "      <th>...</th>\n",
       "      <th>L3_NO2_cloud_fraction</th>\n",
       "      <th>L3_CO_CO_column_number_density</th>\n",
       "      <th>L3_CO_cloud_height</th>\n",
       "      <th>L3_HCHO_tropospheric_HCHO_column_number_density</th>\n",
       "      <th>L3_HCHO_cloud_fraction</th>\n",
       "      <th>L3_O3_O3_column_number_density</th>\n",
       "      <th>L3_O3_cloud_fraction</th>\n",
       "      <th>L3_SO2_SO2_column_number_density</th>\n",
       "      <th>L3_SO2_absorbing_aerosol_index</th>\n",
       "      <th>L3_SO2_cloud_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>010Q650</td>\n",
       "      <td>38.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>769.50</td>\n",
       "      <td>92</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>60.200001</td>\n",
       "      <td>0.00804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006507</td>\n",
       "      <td>0.021080</td>\n",
       "      <td>267.017184</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>-1.861476</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>010Q650</td>\n",
       "      <td>39.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1319.85</td>\n",
       "      <td>91</td>\n",
       "      <td>14.600000</td>\n",
       "      <td>48.799999</td>\n",
       "      <td>0.00839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018360</td>\n",
       "      <td>0.022017</td>\n",
       "      <td>61.216687</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.059433</td>\n",
       "      <td>0.115179</td>\n",
       "      <td>0.059433</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>-1.452612</td>\n",
       "      <td>0.059433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>010Q650</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1181.96</td>\n",
       "      <td>96</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>33.400002</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015904</td>\n",
       "      <td>0.020677</td>\n",
       "      <td>134.700335</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.082063</td>\n",
       "      <td>0.115876</td>\n",
       "      <td>0.082063</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>-1.572950</td>\n",
       "      <td>0.082063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>010Q650</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1113.67</td>\n",
       "      <td>96</td>\n",
       "      <td>6.911948</td>\n",
       "      <td>21.300001</td>\n",
       "      <td>0.00391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055765</td>\n",
       "      <td>0.021207</td>\n",
       "      <td>474.821444</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.121261</td>\n",
       "      <td>0.141557</td>\n",
       "      <td>0.121261</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>-1.239317</td>\n",
       "      <td>0.121261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>010Q650</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1164.82</td>\n",
       "      <td>95</td>\n",
       "      <td>13.900001</td>\n",
       "      <td>44.700001</td>\n",
       "      <td>0.00535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028530</td>\n",
       "      <td>0.037766</td>\n",
       "      <td>926.926310</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.037919</td>\n",
       "      <td>0.126369</td>\n",
       "      <td>0.037919</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.202489</td>\n",
       "      <td>0.037919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30552</th>\n",
       "      <td>2020-03-15</td>\n",
       "      <td>YWSFY6Q</td>\n",
       "      <td>22.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>3848.86</td>\n",
       "      <td>72</td>\n",
       "      <td>6.700000</td>\n",
       "      <td>68.300003</td>\n",
       "      <td>0.00352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.039941</td>\n",
       "      <td>192.388239</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.174995</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>-1.953480</td>\n",
       "      <td>0.001310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30553</th>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>YWSFY6Q</td>\n",
       "      <td>53.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>9823.87</td>\n",
       "      <td>72</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>77.700005</td>\n",
       "      <td>0.00341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004726</td>\n",
       "      <td>0.037872</td>\n",
       "      <td>61.379434</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.007644</td>\n",
       "      <td>0.157659</td>\n",
       "      <td>0.007644</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>-2.178236</td>\n",
       "      <td>0.007644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30554</th>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>YWSFY6Q</td>\n",
       "      <td>85.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>8900.85</td>\n",
       "      <td>72</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>68.500000</td>\n",
       "      <td>0.00356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026249</td>\n",
       "      <td>0.038539</td>\n",
       "      <td>1572.596434</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.025447</td>\n",
       "      <td>0.168295</td>\n",
       "      <td>0.025447</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>-2.365827</td>\n",
       "      <td>0.025447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30555</th>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>YWSFY6Q</td>\n",
       "      <td>103.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>13963.90</td>\n",
       "      <td>72</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>66.300003</td>\n",
       "      <td>0.00523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144318</td>\n",
       "      <td>0.038757</td>\n",
       "      <td>846.961465</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.103292</td>\n",
       "      <td>0.160637</td>\n",
       "      <td>0.173391</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-2.784346</td>\n",
       "      <td>0.153445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30556</th>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>YWSFY6Q</td>\n",
       "      <td>89.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9393.64</td>\n",
       "      <td>72</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>68.400002</td>\n",
       "      <td>0.00486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30557 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date Place_ID  target  target_min  target_max  target_variance  \\\n",
       "0      2020-01-02  010Q650    38.0        23.0        53.0           769.50   \n",
       "1      2020-01-03  010Q650    39.0        25.0        63.0          1319.85   \n",
       "2      2020-01-04  010Q650    24.0         8.0        56.0          1181.96   \n",
       "3      2020-01-05  010Q650    49.0        10.0        55.0          1113.67   \n",
       "4      2020-01-06  010Q650    21.0         9.0        52.0          1164.82   \n",
       "...           ...      ...     ...         ...         ...              ...   \n",
       "30552  2020-03-15  YWSFY6Q    22.0        14.0        83.0          3848.86   \n",
       "30553  2020-03-16  YWSFY6Q    53.0        30.0       146.0          9823.87   \n",
       "30554  2020-03-17  YWSFY6Q    85.0        52.0       153.0          8900.85   \n",
       "30555  2020-03-18  YWSFY6Q   103.0        33.0       149.0         13963.90   \n",
       "30556  2020-03-19  YWSFY6Q    89.0        46.0       132.0          9393.64   \n",
       "\n",
       "       target_count  precipitable_water_entire_atmosphere  \\\n",
       "0                92                             11.000000   \n",
       "1                91                             14.600000   \n",
       "2                96                             16.400000   \n",
       "3                96                              6.911948   \n",
       "4                95                             13.900001   \n",
       "...             ...                                   ...   \n",
       "30552            72                              6.700000   \n",
       "30553            72                              6.300000   \n",
       "30554            72                              7.100000   \n",
       "30555            72                             19.100000   \n",
       "30556            72                             11.600000   \n",
       "\n",
       "       relative_humidity_2m_above_ground  specific_humidity_2m_above_ground  \\\n",
       "0                              60.200001                            0.00804   \n",
       "1                              48.799999                            0.00839   \n",
       "2                              33.400002                            0.00750   \n",
       "3                              21.300001                            0.00391   \n",
       "4                              44.700001                            0.00535   \n",
       "...                                  ...                                ...   \n",
       "30552                          68.300003                            0.00352   \n",
       "30553                          77.700005                            0.00341   \n",
       "30554                          68.500000                            0.00356   \n",
       "30555                          66.300003                            0.00523   \n",
       "30556                          68.400002                            0.00486   \n",
       "\n",
       "       ...  L3_NO2_cloud_fraction  L3_CO_CO_column_number_density  \\\n",
       "0      ...               0.006507                        0.021080   \n",
       "1      ...               0.018360                        0.022017   \n",
       "2      ...               0.015904                        0.020677   \n",
       "3      ...               0.055765                        0.021207   \n",
       "4      ...               0.028530                        0.037766   \n",
       "...    ...                    ...                             ...   \n",
       "30552  ...               0.001107                        0.039941   \n",
       "30553  ...               0.004726                        0.037872   \n",
       "30554  ...               0.026249                        0.038539   \n",
       "30555  ...               0.144318                        0.038757   \n",
       "30556  ...               0.030848                             NaN   \n",
       "\n",
       "       L3_CO_cloud_height  L3_HCHO_tropospheric_HCHO_column_number_density  \\\n",
       "0              267.017184                                         0.000064   \n",
       "1               61.216687                                         0.000171   \n",
       "2              134.700335                                         0.000124   \n",
       "3              474.821444                                         0.000081   \n",
       "4              926.926310                                         0.000140   \n",
       "...                   ...                                              ...   \n",
       "30552          192.388239                                         0.000024   \n",
       "30553           61.379434                                        -0.000014   \n",
       "30554         1572.596434                                         0.000094   \n",
       "30555          846.961465                                         0.000063   \n",
       "30556                 NaN                                              NaN   \n",
       "\n",
       "       L3_HCHO_cloud_fraction  L3_O3_O3_column_number_density  \\\n",
       "0                    0.000000                        0.119095   \n",
       "1                    0.059433                        0.115179   \n",
       "2                    0.082063                        0.115876   \n",
       "3                    0.121261                        0.141557   \n",
       "4                    0.037919                        0.126369   \n",
       "...                       ...                             ...   \n",
       "30552                0.001310                        0.174995   \n",
       "30553                0.007644                        0.157659   \n",
       "30554                0.025447                        0.168295   \n",
       "30555                0.103292                        0.160637   \n",
       "30556                     NaN                             NaN   \n",
       "\n",
       "       L3_O3_cloud_fraction  L3_SO2_SO2_column_number_density  \\\n",
       "0                  0.000000                         -0.000127   \n",
       "1                  0.059433                          0.000150   \n",
       "2                  0.082063                          0.000150   \n",
       "3                  0.121261                          0.000227   \n",
       "4                  0.037919                          0.000390   \n",
       "...                     ...                               ...   \n",
       "30552              0.001310                          0.000312   \n",
       "30553              0.007644                          0.000362   \n",
       "30554              0.025447                          0.000107   \n",
       "30555              0.173391                          0.000014   \n",
       "30556                   NaN                               NaN   \n",
       "\n",
       "       L3_SO2_absorbing_aerosol_index  L3_SO2_cloud_fraction  \n",
       "0                           -1.861476               0.000000  \n",
       "1                           -1.452612               0.059433  \n",
       "2                           -1.572950               0.082063  \n",
       "3                           -1.239317               0.121261  \n",
       "4                            0.202489               0.037919  \n",
       "...                               ...                    ...  \n",
       "30552                       -1.953480               0.001310  \n",
       "30553                       -2.178236               0.007644  \n",
       "30554                       -2.365827               0.025447  \n",
       "30555                       -2.784346               0.153445  \n",
       "30556                             NaN                    NaN  \n",
       "\n",
       "[30557 rows x 29 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_pickle(\"df_train_1.pkl\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30da5a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    30557.000000\n",
       "mean         3.100731\n",
       "std          2.209016\n",
       "min          0.020040\n",
       "25%          1.497562\n",
       "50%          2.545925\n",
       "75%          4.152956\n",
       "max         18.160623\n",
       "Name: windstrength, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create windstrength column\n",
    "df['windstrength'] = np.sqrt(df.u_component_of_wind_10m_above_ground**2 + df.v_component_of_wind_10m_above_ground**2)\n",
    "df.windstrength.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64212f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, date, time, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc66761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change date to datetime\n",
    "df.Date = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "\n",
    "# change date to datetime\n",
    "df['weekday'] = df.Date.dt.dayofweek\n",
    "\n",
    "# Dropping the unnecessary columns \n",
    "df.drop(['Date',\n",
    "         #'Place_ID',\n",
    "         'target_min',\n",
    "         'target_max',\n",
    "         'target_variance',\n",
    "         'target_count',\n",
    "         'u_component_of_wind_10m_above_ground',\n",
    "         'v_component_of_wind_10m_above_ground',\n",
    "         ], axis=1, inplace=True)\n",
    "df.columns\n",
    "\n",
    "df.dropna(thresh=len(df.columns)-8, inplace=True)\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df[\"target\"] = pd.cut(df.target, bins=[0, 35, 115, 850], labels=['1', '2', '3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65e3542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list for categorical predictors/features \n",
    "# (dates are also objects so if you have them in your data you would deal with them first)\n",
    "cat_features = list(df.columns[df.dtypes==object])\n",
    "\n",
    "\n",
    "# Creating list for categorical predictors/features \n",
    "# (dates are also objects so if you have them in your data you would deal with them first)\n",
    "time_features = ['weekday']\n",
    "\n",
    "\n",
    "# Creating list for numerical predictors/features\n",
    "# Since 'target' is our target variable we will exclude this feature from this list of numerical predictors \n",
    "num_features = [#'target_min', 'target_max', 'target_variance', 'target_count',\n",
    "       'precipitable_water_entire_atmosphere',\n",
    "       'relative_humidity_2m_above_ground',\n",
    "       'specific_humidity_2m_above_ground',\n",
    "       'temperature_2m_above_ground',\n",
    "       #'u_component_of_wind_10m_above_ground',\n",
    "       #'v_component_of_wind_10m_above_ground'\n",
    "       'windstrength'\n",
    "       ]\n",
    "\n",
    "\n",
    "# Creating list for numerical predictors/features\n",
    "# Since 'outcome' is our target variable we will exclude this feature from this list of numerical predictors \n",
    "miss_features = ['L3_AER_AI_absorbing_aerosol_index', \n",
    "       'L3_CLOUD_cloud_base_height',\n",
    "       'L3_CLOUD_cloud_fraction', \n",
    "       'L3_CLOUD_cloud_optical_depth',\n",
    "       'L3_NO2_NO2_column_number_density',\n",
    "       'L3_NO2_cloud_fraction', \n",
    "       'L3_CO_cloud_height',\n",
    "       'L3_HCHO_tropospheric_HCHO_column_number_density',\n",
    "       'L3_HCHO_cloud_fraction', \n",
    "       'L3_SO2_SO2_column_number_density',\n",
    "       'L3_SO2_cloud_fraction']\n",
    "\n",
    "\n",
    "replace_features = [\"L3_NO2_absorbing_aerosol_index\",\n",
    "                    \"L3_CO_CO_column_number_density\",\n",
    "                    \"L3_O3_O3_column_number_density\",\n",
    "                    \"L3_O3_cloud_fraction\",\n",
    "                    \"L3_SO2_absorbing_aerosol_index\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e618cc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Place_ID': <KerasTensor: shape=(None,) dtype=string (created by layer 'Place_ID')>, 'target': <KerasTensor: shape=(None,) dtype=string (created by layer 'target')>, 'precipitable_water_entire_atmosphere': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'precipitable_water_entire_atmosphere')>, 'relative_humidity_2m_above_ground': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'relative_humidity_2m_above_ground')>, 'specific_humidity_2m_above_ground': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'specific_humidity_2m_above_ground')>, 'temperature_2m_above_ground': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'temperature_2m_above_ground')>, 'L3_AER_AI_absorbing_aerosol_index': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'L3_AER_AI_absorbing_aerosol_index')>, 'L3_CLOUD_cloud_base_height': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'L3_CLOUD_cloud_base_height')>, 'L3_CLOUD_cloud_fraction': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'L3_CLOUD_cloud_fraction')>, 'L3_CLOUD_cloud_optical_depth': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'L3_CLOUD_cloud_optical_depth')>, 'L3_NO2_NO2_column_number_density': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'L3_NO2_NO2_column_number_density')>, 'L3_NO2_absorbing_aerosol_index': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'L3_NO2_absorbing_aerosol_index')>, 'L3_NO2_cloud_fraction': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'L3_NO2_cloud_fraction')>, 'L3_CO_CO_column_number_density': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'L3_CO_CO_column_number_density')>, 'L3_CO_cloud_height': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'L3_CO_cloud_height')>, 'L3_HCHO_tropospheric_HCHO_column_number_density': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'L3_HCHO_tropospheric_HCHO_column_number_density')>, 'L3_HCHO_cloud_fraction': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'L3_HCHO_cloud_fraction')>, 'L3_O3_O3_column_number_density': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'L3_O3_O3_column_number_density')>, 'L3_O3_cloud_fraction': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'L3_O3_cloud_fraction')>, 'L3_SO2_SO2_column_number_density': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'L3_SO2_SO2_column_number_density')>, 'L3_SO2_absorbing_aerosol_index': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'L3_SO2_absorbing_aerosol_index')>, 'L3_SO2_cloud_fraction': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'L3_SO2_cloud_fraction')>, 'windstrength': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'windstrength')>, 'weekday': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'weekday')>}\n"
     ]
    }
   ],
   "source": [
    "def create_tensor_dict(df, cat_features):\n",
    "    inputs = {}\n",
    "    for name, column in df.items():\n",
    "      if type(column[0]) == str:\n",
    "        dtype = tf.string\n",
    "      elif (name in cat_features):\n",
    "        dtype = tf.int64\n",
    "      else:\n",
    "        dtype = tf.float32\n",
    "\n",
    "      inputs[name] = tf.keras.Input(shape=(), name=name, dtype=dtype)\n",
    "    return inputs\n",
    "\n",
    "inputs = create_tensor_dict(df, cat_features)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9a1e220c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 28613 observations in our dataset and 23 features\n",
      "Our target vector has also 28613 values\n"
     ]
    }
   ],
   "source": [
    "# Define predictors and target variable\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "print(f\"We have {X.shape[0]} observations in our dataset and {X.shape[1]} features\")\n",
    "print(f\"Our target vector has also {y.shape[0]} values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "105baef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26098865",
   "metadata": {},
   "source": [
    "## Training\n",
    "For training you need a train/val split (hopefully you did a train/test split before (and you should use the same as in your ML project to make results comparable). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "467c0dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary to store results\n",
    "training_history = {}\n",
    "\n",
    "# Define number of epochs and learning rate decay\n",
    "N_TRAIN = len(X_train)\n",
    "N_VAL = 0.2\n",
    "EPOCHS = 2000\n",
    "BATCH_SIZE = N_TRAIN // 10\n",
    "STEPS_PER_EPOCH = N_TRAIN // BATCH_SIZE\n",
    "# lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "#     0.01,\n",
    "#     decay_steps=STEPS_PER_EPOCH*1000,\n",
    "#     decay_rate=1,\n",
    "#     staircase=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afda48b9",
   "metadata": {},
   "source": [
    "### Build, compile and fit your model\n",
    "To become fast at retraining your (different) models it is good practice to define a function that gets fed by a model, its name, an optimizer to use and the number of epochs you want the model to be trained. \n",
    "\n",
    "If your model trains for many epochs you will receive a lot of logging from TensorFlow. To reduce the logging noise you can use a callback (provided by the `tensorflow_docs` module we installed and imported for you) named `EpochDots()` that simply prints a `.` for each epoch and a full set of metrics after a number of epochs have been trained. \n",
    "\n",
    "If you want to produce logs for using TensorBoard you also need to include the `callbacks.TensorBoard()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e993234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testeddate = '4/25/2015'\n",
    "#dt_obj = datetime.datetime.strptime(testeddate,'%m/%d/%Y')\n",
    "from datetime import datetime\n",
    "# Define path for new directory \n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "# Define function for creating a new folder for each run\n",
    "def get_run_logdir():\n",
    "    now = datetime.now()\n",
    "    run_id = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ce0a2e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_callbacks():\n",
    "# Define path where checkpoints should be stored\n",
    "checkpoint_path = \"training_1/ML_model.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=0) # Set verbose != 0 if you want output during training \n",
    "# return [list of your callbacks]\n",
    "def get_callbacks(name):\n",
    "    return tf.keras.callbacks.TensorBoard(run_logdir+name, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfcdd8a",
   "metadata": {},
   "source": [
    "You can implement your callbacks in the `model.fit()` method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f042f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_compile_and_fit(model, optimizer=None, max_epochs=EPOCHS):\n",
    "    # Get optimizer\n",
    "   #optimizer = tf.keras.optimizers.Adam(, name='Adam')\n",
    "    # model.compile\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    # model.fit\n",
    "    model.fit(X_train_transformed, y_train, batch_size = BATCH_SIZE, validation_split=N_VAL, epochs = max_epochs)\n",
    "    # return results\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b436f486",
   "metadata": {},
   "source": [
    "#### Build your model\n",
    "You can build your model by using `tf.keras.Sequential()` that helps you to sequentially define your different layers from input to output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "535bfd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu', input_dim = 368),\n",
    "    tf.keras.layers.Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'),\n",
    "    tf.keras.layers.Dense(units= 1,kernel_initializer = 'uniform', activation = 'sigmoid')\n",
    "])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "141b8392",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31056e5",
   "metadata": {},
   "source": [
    "#### Train your model\n",
    "Train your model by using your `model_compile_and_fit()` function you defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a44acb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Pipline for numerical features\n",
    "# Initiating Pipeline and calling one step after another\n",
    "# each step is built as a list of (key, value)\n",
    "# key is the name of the processing step\n",
    "# value is an estimator object (processing step)\n",
    "num_pipeline = Pipeline([\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "# Pipeline for missing values: in this case missing values are nan!\n",
    "miss_pipeline = Pipeline([\n",
    "    ('imputer_num', SimpleImputer(strategy='median',missing_values=np.nan)),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "# replace 0 values\n",
    "# Pipeline for missing values: in this case missing values are 0!\n",
    "replace_pipeline = Pipeline([\n",
    "    ('imputer_null', SimpleImputer(strategy='constant', fill_value = 0)),\n",
    "    ('imputer_nan', SimpleImputer(strategy='median', missing_values= 0)),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features \n",
    "cat_pipeline = Pipeline([\n",
    "    #('imputer_cat', SimpleImputer(strategy='constant',fill_value='missing')),\n",
    "    ('1hot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features \n",
    "time_pipeline = Pipeline([\n",
    "    #('imputer_cat', SimpleImputer(strategy='constant',fill_value='missing')),\n",
    "    ('1hot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "#from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Complete pipeline for numerical and categorical features\n",
    "# 'ColumnTranformer' applies transformers (num_pipeline/ cat_pipeline)\n",
    "# to specific columns of an array or DataFrame (num_features/cat_features)\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features),\n",
    "    ('miss', miss_pipeline, miss_features),\n",
    "    ('replace', replace_pipeline, replace_features),\n",
    "    ('time', time_pipeline, time_features)\n",
    "    ], sparse_threshold=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "93a5d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9ca0549b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20029, 368)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0df56c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot assign a device for operation sequential_4/dense_12/MatMul/ReadVariableOp: Could not satisfy explicit device specification '' because the node {{colocation_node sequential_4/dense_12/MatMul/ReadVariableOp}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0]. \nColocation Debug Info:\nColocation group had the following types and supported devices: \nRoot Member(assigned_device_name_index_=2 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\nReadVariableOp: GPU CPU \nResourceApplyAdam: CPU \n_Arg: GPU CPU \n\nColocation members, user-requested devices, and framework assigned devices, if any:\n  sequential_4_dense_12_matmul_readvariableop_resource (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n  adam_adam_update_resourceapplyadam_m (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n  adam_adam_update_resourceapplyadam_v (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n  sequential_4/dense_12/MatMul/ReadVariableOp (ReadVariableOp) \n  Adam/Adam/update/ResourceApplyAdam (ResourceApplyAdam) /job:localhost/replica:0/task:0/device:GPU:0\n\n\t [[{{node sequential_4/dense_12/MatMul/ReadVariableOp}}]] [Op:__inference_train_function_1343]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#your_history = model_compile_and_fit(your_model, ....)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m training_history \u001b[39m=\u001b[39m model_compile_and_fit(model)\n",
      "Cell \u001b[0;32mIn[70], line 7\u001b[0m, in \u001b[0;36mmodel_compile_and_fit\u001b[0;34m(model, optimizer, max_epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m \u001b[39m# model.fit\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train_transformed, y_train, batch_size \u001b[39m=\u001b[39;49m BATCH_SIZE, validation_split\u001b[39m=\u001b[39;49mN_VAL, epochs \u001b[39m=\u001b[39;49m max_epochs)\n\u001b[1;32m      8\u001b[0m \u001b[39m# return results\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Neue_Fische/ds-artificial-neural-networks/.venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Neue_Fische/ds-artificial-neural-networks/.venv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device for operation sequential_4/dense_12/MatMul/ReadVariableOp: Could not satisfy explicit device specification '' because the node {{colocation_node sequential_4/dense_12/MatMul/ReadVariableOp}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0]. \nColocation Debug Info:\nColocation group had the following types and supported devices: \nRoot Member(assigned_device_name_index_=2 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\nReadVariableOp: GPU CPU \nResourceApplyAdam: CPU \n_Arg: GPU CPU \n\nColocation members, user-requested devices, and framework assigned devices, if any:\n  sequential_4_dense_12_matmul_readvariableop_resource (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n  adam_adam_update_resourceapplyadam_m (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n  adam_adam_update_resourceapplyadam_v (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n  sequential_4/dense_12/MatMul/ReadVariableOp (ReadVariableOp) \n  Adam/Adam/update/ResourceApplyAdam (ResourceApplyAdam) /job:localhost/replica:0/task:0/device:GPU:0\n\n\t [[{{node sequential_4/dense_12/MatMul/ReadVariableOp}}]] [Op:__inference_train_function_1343]"
     ]
    }
   ],
   "source": [
    "#your_history = model_compile_and_fit(your_model, ....)\n",
    "training_history = model_compile_and_fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a0e282",
   "metadata": {},
   "source": [
    "#### Evaluate your model training\n",
    "TensorFlow offers now (this was more cumbersome before) a simple history plotter that you can use to plot training histories and see how the model performed on training and validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4554e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_plotter = tfdocs.plots.HistoryPlotter(metric = 'your_metric', smoothing_std=10)\n",
    "# history_plotter.plot(your_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85938cce",
   "metadata": {},
   "source": [
    "## Model tuning\n",
    "You might have no luck with your first model (most surely you did not). In this section you will apply methods you know to tune your model's performance. An obvious way of course is to change your model's architecture (removing or adding layers or layer dimensions, changing activation functions). \n",
    "\n",
    "However, after this you might still be able to detect some overfitting and there are some more methods you can apply to improve your neural network. Some of them are regularization, learning rate decay, early stopping, or dropout. \n",
    "\n",
    "If you want to add regularization you can apply directly layer-wise L2- or L1-regularization by using a layer's `kernel_regularization` argument and an appropriate regularizer from the [`tensorflow.keras.regularizers`](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers) module we imported for you.  \n",
    "\n",
    "__Optimizer schedules__<br>\n",
    "Quite often your optimizer does not run efficiently through the loss function surface. Remember that theory ensures a convergence of mini-batch SGD if and only if the learing rate decreases sufficiently fast. A way to apply this to your model training is to use a learning rate scheduler (learning rate decay) that reduces the learning rate over the number of update steps. The [`tf.keras.optimizers.schedules`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules) module offers you some approaches to do that. \n",
    "\n",
    "Note that to apply this to your `model_compile_and_fit()` function you defined above you need to implement the learning rate schedule either in there or with a helper function that your function calls inside. \n",
    "\n",
    "If you want to visualize different schedulers you can define them and call them on a range of values and plot them in a line plot. \n",
    "\n",
    "__Early stopping__<br>\n",
    "Earyl stopping is a procedure that enables you to stop your training earlier than defined by your `max_epochs` argument. It is used in practices to \n",
    "1. determine the optimal parameter vector by monitoring the validation error closely (if it rises again too much stuck with the best parameters found until then) and\n",
    "2. to save expensive resources (either in terms of monetary costs or ecological costs).\n",
    "\n",
    "To implement early stopping in TensorFlow the `tf.keras` module offers you a `callback` named [`tf.keras.callback.EarlyStopping()`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) that monitors for you a certain metric (it makes sense here to use a validation metric) and to stop training after a certain number of epochs with no improvement or by defining a certain `min_delta` that defines a minimum value of improvement - if below the callback stops your training. \n",
    "\n",
    "You can add this callback simply to the callbacks defined in your `get_callbacks()` function you defined above.\n",
    "\n",
    "__Dropout__<br>\n",
    "Dropout was one of the important developments in regularization for neural networks. It was developed by Geoffrey Hinton and his team at Toronto University. \n",
    "\n",
    "Dropout can be applied to each layer in your network and is implemented in `tf.keras` by an own layer named `Dropout()` awaiting a dropout rate set by you. So to introduce dropout you have to rework your model design.  \n",
    "\n",
    "Make use of your knowledge and apply tuning techniques to improve your network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3acc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========#\n",
    "# YOUR CODE #\n",
    "#===========#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25560d3a",
   "metadata": {},
   "source": [
    "## Model ensembling\n",
    "You have learned that models can be ensembled. What is possible in `scikit-learn` is also possible in TensorFlow, just a little different as it is relying on its computation graph. However, any model is callable like a `layer` by invoking it on either an `Input` or on the output of another layer. Furthermore, you can also stack outputs together.\n",
    "\n",
    "To produce an ensemble you can define a couple of models, than use their predictions as inputs for another model and produce a final output (using `keras.Model(input, output)`). But you can also start simple and use the mean predictions over all models and then compute the `argmax()` to assign them to a class in classification (via using `layers.average([model1_preds,model2_preds,...])`). You will be surprised how well this works. \n",
    "\n",
    "Now implement your own ensemble to improve your work even a little more and to have something more to polish up your ML project on `GitHub` ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cde4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========#\n",
    "# YOUR CODE #\n",
    "#===========#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "04d07fc6b619cc7b03e19aa1a94701527f1cc8cd0e8ddfc8c64e4286a37b44b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
